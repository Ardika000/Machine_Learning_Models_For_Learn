{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4954a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from os import listdir\n",
    "from os.path import isfile, join, normpath\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0e3e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 26\n",
    "TEST_SIZE = 0.3\n",
    "N_MFCC = 58\n",
    "LS_CSV_PATH = \"../data/HLS-CMDS/LS.csv\"\n",
    "LS_AUDIO_PATH = \"../data/HLS-CMDS/LS/LS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5f84f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files found: 52\n",
      "First 5 audio files: ['.DS_Store', 'F_CC_LLA.wav', 'F_CC_LMA.wav', 'F_CC_LUA.wav', 'F_CC_RLA.wav']\n"
     ]
    }
   ],
   "source": [
    "audioFiles = [f for f in listdir(LS_AUDIO_PATH) if isfile(join(LS_AUDIO_PATH, f))]\n",
    "print(f\"Number of audio files found: {len(audioFiles)}\")\n",
    "print(f\"First 5 audio files: {audioFiles[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a172adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files found: 50\n",
      "First 5 .wav files: ['F_CC_LLA.wav', 'F_CC_LMA.wav', 'F_CC_LUA.wav', 'F_CC_RLA.wav', 'F_CC_RMA.wav']\n"
     ]
    }
   ],
   "source": [
    "wavfiles = [f for f in audioFiles if f.endswith('.wav')]\n",
    "print(f\"Number of .wav files found: {len(wavfiles)}\")\n",
    "print(f\"First 5 .wav files: {wavfiles[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fdba1955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .wav files found: 50\n",
      "First 5 .wav files: ['../data/HLS-CMDS/LS/LS/F_CC_LLA.wav', '../data/HLS-CMDS/LS/LS/F_CC_LMA.wav', '../data/HLS-CMDS/LS/LS/F_CC_LUA.wav', '../data/HLS-CMDS/LS/LS/F_CC_RLA.wav', '../data/HLS-CMDS/LS/LS/F_CC_RMA.wav']\n"
     ]
    }
   ],
   "source": [
    "wavfiles = [LS_AUDIO_PATH + '/' + f for f in wavfiles]\n",
    "print(f\"Number of .wav files found: {len(wavfiles)}\")\n",
    "print(f\"First 5 .wav files: {wavfiles[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7bc9e419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gender Lung Sound Type Location Lung Sound ID\n",
      "0      M          Normal      RUA       M_N_RUA\n",
      "1      F          Normal      LUA       F_N_LUA\n",
      "2      F          Normal      RMA       F_N_RMA\n",
      "3      F          Normal      LMA       F_N_LMA\n",
      "4      M          Normal      RLA       M_N_RLA\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(LS_CSV_PATH)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14aa7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_path'] = df['Lung Sound ID'].apply(lambda x: normpath(join(LS_AUDIO_PATH, f\"{str(x).strip()}.wav\").replace('\\\\', '/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7f2d1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['audio_path'] = df['audio_path'].str.replace('_C_', '_FC_').str.replace('_G_', '_CC_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80ee3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(filename, n_mfcc=N_MFCC):\n",
    "    y, sr = librosa.load(filename, sr=44100)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfcc_mean = np.mean(mfcc, axis=1)\n",
    "    mfcc_std = np.std(mfcc, axis=1)\n",
    "    return np.concatenate([mfcc_mean, mfcc_std])\n",
    "\n",
    "def flatten_mfcc(mfcc):\n",
    "    return np.array(mfcc).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a4944b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_list = []\n",
    "labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    try:\n",
    "        mfcc = extract_mfcc(row['audio_path'])\n",
    "        # mfcc_flat = flatten_mfcc(mfcc)\n",
    "        mfcc_list.append(mfcc)\n",
    "        labels.append(row['Gender'])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed: {row['audio_path']} -> {e}\")\n",
    "    \n",
    "X = pd.DataFrame(mfcc_list)\n",
    "y = pd.Series(labels)\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d7c948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf', random_state=RANDOM_STATE, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'svc__C': [0.1, 1, 10, 100],\n",
    "    'svc__gamma': [0.001, 0.01, 0.1, 1],\n",
    "    'svc__kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "model_grid = GridSearchCV(\n",
    "    estimator=pipe_model, \n",
    "    param_grid=param_grid, \n",
    "    scoring='f1_macro', \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "992df40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'svc__C': 0.1, 'svc__gamma': 0.1, 'svc__kernel': 'poly'}\n",
      "LS: Gender -> macro-F1: 0.5982142857142857\n"
     ]
    }
   ],
   "source": [
    "model_grid.fit(Xtrain, ytrain)\n",
    "model = model_grid.best_estimator_\n",
    "print(f\"Best parameters: {model_grid.best_params_}\")\n",
    "\n",
    "ypred = model.predict(Xtest)\n",
    "f1 = f1_score(ytest, ypred, average='macro')\n",
    "print(f\"LS: Gender -> macro-F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9fe584fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4 2]\n",
      " [4 5]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.50      0.67      0.57         6\n",
      "           M       0.71      0.56      0.62         9\n",
      "\n",
      "    accuracy                           0.60        15\n",
      "   macro avg       0.61      0.61      0.60        15\n",
      "weighted avg       0.63      0.60      0.60        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(ytest, ypred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(ytest, ypred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ExamMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
